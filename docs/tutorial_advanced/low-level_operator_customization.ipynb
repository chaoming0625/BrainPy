{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Low-level Operator Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "@[Tianqiu Zhang](https://github.com/ztqakita)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "BrainPy is built on Jax and can accelerate model running performance based on [Just-in-Time(JIT) compilation](./compilation.ipynb). In order to enhance performance on CPU and GPU, we publish another package ``BrainPyLib`` to provide several built-in low-level operators in synaptic computation. These operators are written in C++ and wrapped as Jax primitives by using ``XLA``. However, users cannot simply customize their own operators unless they have specific background. To solve this problem, we introduce `numba.cfunc` here and provide convenient interfaces for users to customize operators without touching the underlying logic."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "from jax.abstract_arrays import ShapedArray\n",
    "\n",
    "bm.set_platform('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In [Computation with Sparse Connections](../tutorial_simulation/synapse_models.ipynb) section, we formally discuss the benefits of computation with our built-in operators. These operators are provided by `brainpylib` package and can be accessed through `brainpy.math` module. To be more specific, in order to speed up sparse synaptic computation, we customize several low-level operators for CPU and GPU, which are written in C++ and converted into Jax/XLA compatible primitive by using `Pybind11`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is not easy to write a C++ operator and implement a series of conversion. Users have to learn how to write a C++ operator, how to write a customized Jax primitive, and how to convert your C++ operator into a Jax primitive. Here are some links for users who prefer to dive into the details: [Jax primitives](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html), [XLA custom calls](https://www.tensorflow.org/xla/custom_call).\n",
    "\n",
    "However, we can only provide limit amounts of operators for users, and it would be great if users can customize their own operators in a relatively simple way. To achieve this goal, BrainPy provides a convenient interface `register_op` to register customized operators on CPU and GPU. Users no longer need to involve any C++ programming and XLA compilation. This is accomplished with the help of [`numba.cfunc`](https://numba.pydata.org/numba-doc/latest/user/cfunc.html), which will wrap python code as a compiled function callable from foreign C code. The C function object exposes the address of the compiled C callback so that it can be passed into XLA and registered as a jittable Jax primitives. Parameters and return types of `register_op` is listed in [this api docs](../apis/auto/math/generated/brainpy.math.operators.register_op.rst). Here is an example of using `register_op` on CPU."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to customize operators?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CPU version\n",
    "\n",
    "First, users can customize a simple operator written in python. Notice that this python operator will be jitted in nopython mode, but some language features are not available inside Numba-compiled functions. Please look up [numba documentations](https://numba.pydata.org/numba-doc/latest/reference/pysupported.html#pysupported) for details."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def custom_op(outs, ins):\n",
    "  y, y1 = outs\n",
    "  x, x2 = ins\n",
    "  y[:] = x + 1\n",
    "  y1[:] = x2 + 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are some restrictions that users should know:\n",
    "- Parameters of the operators are `outs` and `ins`, corresponding to output variable(s) and input variable(s). The order cannot be changed.\n",
    "- The function cannot have any return value.\n",
    "- Notice that in GPU version users should write kernel function according to [numba cuda.jit documentation](https://numba.pydata.org/numba-doc/latest/cuda/index.html). When applying CPU function to GPU, users only need to implement CPU operators."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then users should describe the shapes and types of the outputs, because jax/python can deduce the shapes and types of inputs when you call it, but it cannot infer the shapes and types of the outputs. The argument can be:\n",
    "- a `ShapedArray`,\n",
    "- a sequence of `ShapedArray`,\n",
    "- a function, it should return correct output shapes of `ShapedArray`.\n",
    "\n",
    "Here we use function to describe the output shapes and types. The arguments include all the inputs of custom operators, but only shapes and types are accessible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def abs_eval_1(*ins):\n",
    "  # ins: inputs arguments, only shapes and types are accessible.\n",
    "  # Because custom_op outputs shapes and types are exactly the\n",
    "  # same as inputs, so here we can only return ordinary inputs.\n",
    "  return ins"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function above is somewhat abstract for users, so here we give an alternative function below for passing shape information. We want you to know ``abs_eval_1`` and ``abs_eval_2`` are doing the same thing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def abs_eval_2(*ins):\n",
    "  return ShapedArray(ins[0].shape, ins[0].dtype), ShapedArray(ins[1].shape, ins[1].dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have prepared for registering a CPU operator. `register_op` will be called to wrap your operator and return a jittable Jax primitives. Here are some parameters users should define:\n",
    "- `op_name`: Name of the operator.\n",
    "- `cpu_func`: Customized operator of CPU version.\n",
    "- `out_shapes`: The shapes and types of the outputs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DeviceArray([[2., 2.]], dtype=float32), DeviceArray([[3., 3.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "z = jnp.ones((1, 2), dtype=jnp.float32)\n",
    "# Users could try out_shapes=abs_eval_2 and see if the result is different\n",
    "op = bm.register_op(\n",
    "  op_name='add',\n",
    "  cpu_func=custom_op,\n",
    "  out_shapes=abs_eval_1,\n",
    "  apply_cpu_func_to_gpu=False)\n",
    "jit_op = jit(op)\n",
    "print(jit_op(z, z))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPU version\n",
    "\n",
    "We have discussed how to customize a CPU operator above, next we will talk about GPU operator, which is slightly different from CPU version. There are two additional parameters users need to provide:\n",
    "- `gpu_func`: Customized operator of CPU version.\n",
    "- `apply_cpu_func_to_gpu`: Whether to run kernel function on CPU for an alternative way for GPU version.\n",
    "\n",
    "```{warning}\n",
    "  GPU operators will be wrapped by `cuda.jit` in `numba`, but `numba` currently is not support to launch CUDA kernels from `cfuncs`. For this reason, `gpu_func` is none for default, and there will be an error if users pass a gpu operator to `gpu_func`.\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Therefore, BrainPy enables users to set `apply_cpu_func_to_gpu` to true for a backup method. All the inputs will be initialized on GPU and transferred to CPU for computing. The operator users have defined will be implemented on CPU and the results will be transferred back to GPU for further tasks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To illustrate the effectiveness of this approach, we will compare the customized operators with BrainPy built-in operators. Here we use `event_sum` as an example. The implementation of `event_sum` by using our customization is shown as below:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def abs_eval(events, indices, indptr, post_size, values):\n",
    "  return post_size\n",
    "\n",
    "\n",
    "def event_sum_op(outs, ins):\n",
    "  post_val = outs\n",
    "  events, indices, indptr, post_size, values = ins\n",
    "\n",
    "  for i in range(len(events)):\n",
    "      if events[i]:\n",
    "        for j in range(indptr[i], indptr[i+1]):\n",
    "          index = indices[j]\n",
    "          old_value = post_val[index]\n",
    "          post_val[index] = values + old_value\n",
    "\n",
    "\n",
    "event_sum = bm.register_op(op_name='event_sum', cpu_func=event_sum_op, out_shapes=abs_eval)\n",
    "jit_event_sum = jit(event_sum)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exponential COBA will be our benchmark for testing the speed. We will use built-in operator `event_sum` first."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ExpCOBA(bp.dyn.TwoEndConn):\n",
    "  def __init__(self, pre, post, conn, g_max=1., delay=0., tau=8.0, E=0.,\n",
    "               method='exp_auto'):\n",
    "    super(ExpCOBA, self).__init__(pre=pre, post=post, conn=conn)\n",
    "    self.check_pre_attrs('spike')\n",
    "    self.check_post_attrs('input', 'V')\n",
    "\n",
    "    # parameters\n",
    "    self.E = E\n",
    "    self.tau = tau\n",
    "    self.delay = delay\n",
    "    self.g_max = g_max\n",
    "    self.pre2post = self.conn.require('pre2post')\n",
    "\n",
    "    # variables\n",
    "    self.g = bm.Variable(bm.zeros(self.post.num))\n",
    "\n",
    "    # function\n",
    "    self.integral = bp.odeint(lambda g, t: -g / self.tau, method=method)\n",
    "\n",
    "  def update(self, _t, _dt):\n",
    "    self.g.value = self.integral(self.g, _t, dt=_dt)\n",
    "    # Built-in operator\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    self.g += bm.pre2post_event_sum(self.pre.spike, self.pre2post, self.post.num, self.g_max)\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    self.post.input += self.g * (self.E - self.post.V)\n",
    "\n",
    "\n",
    "class EINet(bp.dyn.Network):\n",
    "  def __init__(self, scale=1.0, method='exp_auto'):\n",
    "    # network size\n",
    "    num_exc = int(3200 * scale)\n",
    "    num_inh = int(800 * scale)\n",
    "\n",
    "    # neurons\n",
    "    pars = dict(V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5.)\n",
    "    E = bp.models.LIF(num_exc, **pars, method=method)\n",
    "    I = bp.models.LIF(num_inh, **pars, method=method)\n",
    "    E.V[:] = bp.math.random.randn(num_exc) * 2 - 55.\n",
    "    I.V[:] = bp.math.random.randn(num_inh) * 2 - 55.\n",
    "\n",
    "    # synapses\n",
    "    we = 0.6 / scale  # excitatory synaptic weight (voltage)\n",
    "    wi = 6.7 / scale  # inhibitory synaptic weight\n",
    "    E2E = ExpCOBA(E, E, bp.conn.FixedProb(prob=0.02), E=0., g_max=we, tau=5., method=method)\n",
    "    E2I = ExpCOBA(E, I, bp.conn.FixedProb(prob=0.02), E=0., g_max=we, tau=5., method=method)\n",
    "    I2E = ExpCOBA(I, E, bp.conn.FixedProb(prob=0.02), E=-80., g_max=wi, tau=10., method=method)\n",
    "    I2I = ExpCOBA(I, I, bp.conn.FixedProb(prob=0.02), E=-80., g_max=wi, tau=10., method=method)\n",
    "\n",
    "    super(EINet, self).__init__(E2E, E2I, I2E, I2I, E=E, I=I)\n",
    "\n",
    "\n",
    "net = EINet(scale=10., method='euler')\n",
    "# simulation\n",
    "runner = bp.dyn.DSRunner(net, inputs=[('E.input', 20.), ('I.input', 20.)])\n",
    "t = runner.run(10000.)\n",
    "print(t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50a6d349210648ddba82a011dba25d7a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.628559827804565\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The total time is 15.62 seconds. Next we use our customized operator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "caf6d0719a994aff9fcb30a65a2de12d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.703513145446777\n"
     ]
    }
   ],
   "source": [
    "class ExpCOBA(bp.dyn.TwoEndConn):\n",
    "  def __init__(self, pre, post, conn, g_max=1., delay=0., tau=8.0, E=0.,\n",
    "               method='exp_auto'):\n",
    "    super(ExpCOBA, self).__init__(pre=pre, post=post, conn=conn)\n",
    "    self.check_pre_attrs('spike')\n",
    "    self.check_post_attrs('input', 'V')\n",
    "\n",
    "    # parameters\n",
    "    self.E = E\n",
    "    self.tau = tau\n",
    "    self.delay = delay\n",
    "    self.g_max = g_max\n",
    "    self.pre2post = self.conn.require('pre2post')\n",
    "\n",
    "    # variables\n",
    "    self.g = bm.Variable(bm.zeros(self.post.num))\n",
    "\n",
    "    # function\n",
    "    self.integral = bp.odeint(lambda g, t: -g / self.tau, method=method)\n",
    "\n",
    "  def update(self, _t, _dt):\n",
    "    self.g.value = self.integral(self.g, _t, dt=_dt)\n",
    "    post_size = bm.zeros(self.post.num)\n",
    "    # Customized operator\n",
    "    # ------------------------------------------------------------------------------------------------------------\n",
    "    self.g += jit_event_sum(self.pre.spike, self.pre2post[0].value, self.pre2post[1].value, post_size, self.g_max)\n",
    "    # ------------------------------------------------------------------------------------------------------------\n",
    "    self.post.input += self.g * (self.E - self.post.V)\n",
    "\n",
    "\n",
    "class EINet(bp.dyn.Network):\n",
    "  def __init__(self, scale=1.0, method='exp_auto'):\n",
    "    # network size\n",
    "    num_exc = int(3200 * scale)\n",
    "    num_inh = int(800 * scale)\n",
    "\n",
    "    # neurons\n",
    "    pars = dict(V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5.)\n",
    "    E = bp.models.LIF(num_exc, **pars, method=method)\n",
    "    I = bp.models.LIF(num_inh, **pars, method=method)\n",
    "    E.V[:] = bp.math.random.randn(num_exc) * 2 - 55.\n",
    "    I.V[:] = bp.math.random.randn(num_inh) * 2 - 55.\n",
    "\n",
    "    # synapses\n",
    "    we = 0.6 / scale  # excitatory synaptic weight (voltage)\n",
    "    wi = 6.7 / scale  # inhibitory synaptic weight\n",
    "    E2E = ExpCOBA(E, E, bp.conn.FixedProb(prob=0.02), E=0., g_max=we, tau=5., method=method)\n",
    "    E2I = ExpCOBA(E, I, bp.conn.FixedProb(prob=0.02), E=0., g_max=we, tau=5., method=method)\n",
    "    I2E = ExpCOBA(I, E, bp.conn.FixedProb(prob=0.02), E=-80., g_max=wi, tau=10., method=method)\n",
    "    I2I = ExpCOBA(I, I, bp.conn.FixedProb(prob=0.02), E=-80., g_max=wi, tau=10., method=method)\n",
    "\n",
    "    super(EINet, self).__init__(E2E, E2I, I2E, I2I, E=E, I=I)\n",
    "\n",
    "\n",
    "net = EINet(scale=10., method='euler')\n",
    "runner = bp.dyn.DSRunner(net, inputs=[('E.input', 20.), ('I.input', 20.)])\n",
    "t = runner.run(10000.)\n",
    "print(t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After comparison, the customization method is almost as fast as the built-in method. Users can simply build their own operators without considering the computation speed loss."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}