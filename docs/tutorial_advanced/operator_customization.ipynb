{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Low-level Operator Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "@[Tianqiu Zhang](https://github.com/ztqakita) @[Chaoming Wang](https://github.com/chaoming0625)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In [Computation with Sparse Connections](../tutorial_simulation/synapse_models.ipynb) section, we formally discuss the benefits of computation with our built-in operators. These operators are provided by `brainpylib` package and can be accessed through `brainpy.math` module. However, these low-level operators for CPU and GPU devices are written in C++ and CUDA. It is not easy to write a C++ operator and implement a series of conversion. Users have to learn how to write a C++ operator, how to write a customized JAX primitive, and how to convert your C++ operator into a JAX primitive. Here are some links for users who prefer to dive into the details: [JAX primitives](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html), [XLA custom calls](https://www.tensorflow.org/xla/custom_call)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, it would be great if users can customize their own operators in a relatively simple way. To achieve this goal, BrainPy provides convenient interfaces ``brainpy.math.register_op()`` and ``brainpy.math.XLACustomOp``  to register customized operators on CPU and GPU devices with Python syntax. Users no longer need to involve any C++ programming and XLA compilation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.abstract_arrays import ShapedArray\n",
    "\n",
    "bm.set_platform('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Customize a CPU operator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The customization of CPU operator is accomplished with the help of [`numba.cfunc`](https://numba.pydata.org/numba-doc/latest/user/cfunc.html), which will wrap python code as a compiled function callable from foreign C code. The C function object exposes the address of the compiled C callback so that it can be passed into XLA and registered as a jittable JAX primitives. Parameters and return types of `register_op` is listed in [this api docs](../apis/auto/math/generated/brainpy.math.operators.register_op.rst)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In general, the customization of a CPU operator needs to provide two function:\n",
    "\n",
    "- **abstract evaluation function**: specifies the abstract shape and dtype of the output according to the input abstract information. This information is used because it can help JAX to infer the shapes and types of the outputs. This *abstract evaluation function* can be provided as\n",
    "  - a `ShapedArray`, like\n",
    "    ```python\n",
    "    ShapedArray(10, jnp.float32)\n",
    "    ```\n",
    "  - a sequence of `ShapedArray`, like\n",
    "    ```python\n",
    "    [ShapedArray(10, jnp.float32), ShapedArray(1, jnp.int32)]\n",
    "    ```\n",
    "  - a function, it should return correct output shapes of `ShapedArray`, like\n",
    "    ```python\n",
    "    def abs_eval(inp1, inp2):\n",
    "      return (ShapedArray(inp1.shape, inp1.dtype),\n",
    "              ShapedArray(inp2.shape, inp2.dtype))\n",
    "    ```\n",
    "\n",
    "- **concreate computation function**: specifies how the output data are computed according to the input data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is an example of operator customization on CPU device."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# What we want to do is a simple add operation.\n",
    "# Therefore, the shape and dtype of outputs are\n",
    "# the same with those of inputs.\n",
    "\n",
    "def abs_eval(*ins):\n",
    "  # ins: inputs arguments, only shapes and types are accessible.\n",
    "  # Because custom_op outputs shapes and types are exactly the\n",
    "  # same as inputs, so here we can only return ordinary inputs.\n",
    "  return ins"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Note here the concreate computation function only supports\n",
    "# to receive two arguments \"outs\" and \"ins\", and does not\n",
    "# support value return.\n",
    "\n",
    "def con_compute(outs, ins):\n",
    "  y, y1 = outs\n",
    "  x, x2 = ins\n",
    "  y[:] = x + 1\n",
    "  y1[:] = x2 + 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are some restrictions for *concreate computation function* that users should know:\n",
    "\n",
    "- Parameters of the operators are `outs` and `ins`, corresponding to output variable(s) and input variable(s). The order cannot be changed.\n",
    "- The function cannot have any return value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have prepared for registering a CPU operator. `register_op` or `XLACustomOp` will be called to wrap your operator and return a jittable JAX primitives. Here are some parameters users should define:\n",
    "- `name`: Name of the operator.\n",
    "- `cpu_func`: Customized operator of CPU version.\n",
    "- `eval_shape`: The shapes and types of the outputs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "op = bm.register_op(name='add',\n",
    "                    cpu_func=con_compute,\n",
    "                    eval_shape=abs_eval)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class AddOp(bm.XLACustomOp):\n",
    "  def __init__(self, name):\n",
    "\n",
    "    def abs_eval(*ins):\n",
    "      return ins\n",
    "\n",
    "    def con_compute(outs, ins):\n",
    "      y, y1 = outs\n",
    "      x, x2 = ins\n",
    "      y[:] = x + 1\n",
    "      y1[:] = x2 + 2\n",
    "\n",
    "    super(AddOp, self).__init__(name=name, cpu_func=con_compute, eval_shape=abs_eval)\n",
    "\n",
    "op2 = AddOp('add')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try to use this operator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[DeviceArray([[2., 2.]], dtype=float32),\n DeviceArray([[3., 3.]], dtype=float32)]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = jnp.ones((1, 2), dtype=jnp.float32)\n",
    "\n",
    "jax.jit(op)(z, z)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[DeviceArray([[2., 2.]], dtype=float32),\n DeviceArray([[3., 3.]], dtype=float32)]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jit(op2)(z, z)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "```{note}\n",
    "\n",
    "Actually, the concreate computation function should be a function compatitable with the nonpython mode of ``numba.jit()``. Users should refer to [Numba's documentation](https://numba.pydata.org/numba-doc/latest/user/jit.html) to check how to write a function which can be jitted by Numba. Fortunately, Numba's JIT support most of the [Python features](https://numba.pydata.org/numba-doc/latest/reference/pysupported.html) and [NumPy features](https://numba.pydata.org/numba-doc/latest/reference/numpysupported.html). This means that this customization interface can be very general to apply on almost all customized computations you want.\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Customize a GPU operator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Customizing operators for GPU devices is extremely hard. We are still working on it. But it will come soon."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Currently, we support to apply CPU function of the operator to the GPU. This is controlled by ``apply_cpu_func_to_gpu=True`` setting during the operator registration. When turn on this option, the input data on the GPU will move to the host CPU for computing. Then the results in the CPU device will be moved back to GPU for other computations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "op3 = bm.register_op(name='add2',\n",
    "                     cpu_func=con_compute,\n",
    "                     eval_shape=abs_eval,\n",
    "                     apply_cpu_func_to_gpu=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmarking the customized operator performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To illustrate the effectiveness of this approach, we will compare the customized operators with BrainPy built-in operators. Here we use `event_sum` as an example. The implementation of `event_sum` by using our customization is shown as below:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Operator customized by using the Python syntax."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class EventSum(bm.XLACustomOp):\n",
    "  \"\"\"Customized operator.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    def abs_eval(events, indices, indptr, post_val, values):\n",
    "      return post_val\n",
    "\n",
    "    def con_compute(outs, ins):\n",
    "      post_val = outs\n",
    "      events, indices, indptr, _, values = ins\n",
    "      for i in range(events.size):\n",
    "        if events[i]:\n",
    "          for j in range(indptr[i], indptr[i + 1]):\n",
    "            index = indices[j]\n",
    "            old_value = post_val[index]\n",
    "            post_val[index] = values + old_value\n",
    "\n",
    "    super(EventSum, self).__init__(eval_shape=abs_eval,\n",
    "                                   con_compute=con_compute)\n",
    "\n",
    "\n",
    "event_sum = EventSum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Exponential synapse model which is implemented through the above Python level operator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "class ExponentialV2(bp.dyn.TwoEndConn):\n",
    "  \"\"\"Exponential synapse model using customized operator written in C++.\"\"\"\n",
    "\n",
    "  def __init__(self, pre, post, conn, g_max=1., delay=0., tau=8.0, E=0.):\n",
    "    super(ExponentialV2, self).__init__(pre=pre, post=post, conn=conn)\n",
    "    self.check_pre_attrs('spike')\n",
    "    self.check_post_attrs('input', 'V')\n",
    "\n",
    "    # parameters\n",
    "    self.E = E\n",
    "    self.tau = tau\n",
    "    self.delay = delay\n",
    "    self.g_max = g_max\n",
    "    self.pre2post = self.conn.require('pre2post')\n",
    "\n",
    "    # variables\n",
    "    self.g = bm.Variable(bm.zeros(self.post.num))\n",
    "\n",
    "    # function\n",
    "    self.integral = bp.odeint(lambda g, t: -g / self.tau, method='exp_auto')\n",
    "\n",
    "  def update(self, tdi):\n",
    "    self.g.value = self.integral(self.g, tdi.t, tdi.dt)\n",
    "    self.g += event_sum(self.pre.spike, self.pre2post[0], self.pre2post[1],\n",
    "                        bm.zeros(self.post.num), self.g_max)\n",
    "    self.post.input += self.g * (self.E - self.post.V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Exponential synapse model which is implemented through the C++ build-in operator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class ExponentialV1(bp.dyn.TwoEndConn):\n",
    "  \"\"\"Exponential synapse model using customized operator written in C++.\"\"\"\n",
    "\n",
    "  def __init__(self, pre, post, conn, g_max=1., delay=0., tau=8.0, E=0.):\n",
    "    super(ExponentialV1, self).__init__(pre=pre, post=post, conn=conn)\n",
    "    self.check_pre_attrs('spike')\n",
    "    self.check_post_attrs('input', 'V')\n",
    "\n",
    "    # parameters\n",
    "    self.E = E\n",
    "    self.tau = tau\n",
    "    self.delay = delay\n",
    "    self.g_max = g_max\n",
    "    self.pre2post = self.conn.require('pre2post')\n",
    "\n",
    "    # variables\n",
    "    self.g = bm.Variable(bm.zeros(self.post.num))\n",
    "\n",
    "    # function\n",
    "    self.integral = bp.odeint(lambda g, t: -g / self.tau, method='exp_auto')\n",
    "\n",
    "  def update(self, tdi):\n",
    "    self.g.value = self.integral(self.g, tdi.t, tdi.dt)\n",
    "    self.g += bm.pre2post_event_sum(self.pre.spike, self.pre2post, self.post.num, self.g_max)\n",
    "    self.post.input += self.g * (self.E - self.post.V)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The E/I balanced network model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class EINet(bp.dyn.Network):\n",
    "  def __init__(self, scale, syn_type='v1'):\n",
    "    syn_cls = ExponentialV1 if syn_type == 'v1' else ExponentialV2\n",
    "\n",
    "    # neurons\n",
    "    pars = dict(V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5.,\n",
    "                V_initializer=bp.init.Normal(-55., 2.))\n",
    "    E = bp.neurons.LIF(int(3200 * scale), **pars, method='exp_auto')\n",
    "    I = bp.neurons.LIF(int(800 * scale), **pars, method='exp_auto')\n",
    "\n",
    "    # synapses\n",
    "    E2E = syn_cls(E, E, bp.conn.FixedProb(prob=0.02), E=0., g_max=0.6 / scale, tau=5.)\n",
    "    E2I = syn_cls(E, I, bp.conn.FixedProb(prob=0.02), E=0., g_max=0.6 / scale, tau=5.)\n",
    "    I2E = syn_cls(I, E, bp.conn.FixedProb(prob=0.02), E=-80., g_max=6.7 / scale, tau=10.)\n",
    "    I2I = syn_cls(I, I, bp.conn.FixedProb(prob=0.02), E=-80., g_max=6.7 / scale, tau=10.)\n",
    "\n",
    "    super(EINet, self).__init__(E2E, E2I, I2E, I2I, E=E, I=I)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's compare the speed results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "642d3b16d7524587a7147485b54e65df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator implemented through C++ : 12.278022527694702\n"
     ]
    }
   ],
   "source": [
    "net1 = EINet(scale=10., syn_type='v1')\n",
    "runner1 = bp.dyn.DSRunner(net1, inputs=[('E.input', 20.), ('I.input', 20.)])\n",
    "t, _ = runner1.predict(10000., eval_time=True)\n",
    "print(\"Operator implemented through C++ :\", t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c34f093e66994b488ae6f97267f2829f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator implemented through Python:  11.629684686660767\n"
     ]
    }
   ],
   "source": [
    "net2 = EINet(scale=10., syn_type='v2')\n",
    "runner2 = bp.dyn.DSRunner(net2, inputs=[('E.input', 20.), ('I.input', 20.)])\n",
    "t, _ = runner2.predict(10000., eval_time=True)\n",
    "print('Operator implemented through Python: ', t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After comparison, the customization method is almost as fast as the built-in method. Users can simply build their own operators without considering the computation speed loss."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}