{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Interoperation with other JAX frameworks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "BrainPy is designed to be easily interoperated with other JAX frameworks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import jax\n",
    "import brainpy as bp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# math library of BrainPy, JAX, NumPy\n",
    "import brainpy.math as bm\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. data are exchangeable among different frameworks.\n",
    "This can be realized because ``Array`` can be direactly converted to JAX ndarray or NumPy ndarray."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert a ``Array`` into a JAX ndarray."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "b = bm.random.randint(10, size=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray([9, 9, 0, 4, 7], dtype=int32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Array.value is a JAX's DeviceArray\n",
    "b.value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert a ``Array`` into a numpy ndarray."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([9, 9, 0, 4, 7])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Array can be easily converted to a numpy ndarray\n",
    "np.asarray(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert a numpy ndarray into a ``Array``."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Array([0, 1, 2, 3, 4], dtype=int32)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.asarray(np.arange(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert a JAX ndarray into a ``Array``."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Array([0, 1, 2, 3, 4], dtype=int32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.asarray(jnp.arange(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Array([0, 1, 2, 3, 4], dtype=int32)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.Array(jnp.arange(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. transformations in ``brainpy.math`` also work on functions.\n",
    "\n",
    "APIs in other JAX frameworks can be naturally integrated in BrainPy. Let's take the gradient-based optimization library [Optax](https://github.com/deepmind/optax) as an example to illustrate how to use other JAX frameworks in BrainPy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import optax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# First create several useful functions.\n",
    "\n",
    "network = jax.vmap(lambda params, x: bm.dot(params, x), in_axes=(None, 0))\n",
    "optimizer = optax.adam(learning_rate=1e-1)\n",
    "\n",
    "def compute_loss(params, x, y):\n",
    "  y_pred = network(params, x)\n",
    "  loss = bm.mean(optax.l2_loss(y_pred, y))\n",
    "  return loss\n",
    "\n",
    "@bm.jit\n",
    "def train(params, opt_state, xs, ys):\n",
    "  grads = bm.grad(compute_loss)(params, xs.value, ys)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "\n",
    "bm.random.seed(42)\n",
    "target_params = 0.5\n",
    "xs = bm.random.normal(size=(16, 2))\n",
    "ys = bm.sum(xs * target_params, axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Initialize parameters of the model + optimizer\n",
    "\n",
    "params = bm.array([0.0, 0.0])\n",
    "opt_state = optimizer.init(params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# A simple update loop\n",
    "\n",
    "for _ in range(1000):\n",
    "  params, opt_state = train(params, opt_state, xs, ys)\n",
    "\n",
    "assert bm.allclose(params, target_params), \\\n",
    "  'Optimization should retrieve the target params used to generate the data.'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. other JAX frameworks can be integrated into a BrainPy program."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we use the [Flax](https://github.com/google/flax), a library used for deep neural networks, to define a convolutional neural network (CNN). The, we integrate this CNN model into our RNN model which defined by BrainPy's syntax."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we first use **flax** to define a CNN network."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  \"\"\"A CNN model implemented by using Flax.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = nn.Dense(features=256)(x)\n",
    "    x = nn.relu(x)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we define an RNN model by using our BrainPy interface."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_flatten, tree_map, tree_unflatten\n",
    "\n",
    "class Network(bp.dyn.DynamicalSystem):\n",
    "  \"\"\"A network model implemented by BrainPy\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Network, self).__init__()\n",
    "\n",
    "    # cnn and its parameters\n",
    "    self.cnn = CNN()\n",
    "    rng = bm.random.DEFAULT.split_key()\n",
    "    params = self.cnn.init(rng, jnp.ones([1, 4, 28, 1]))['params']\n",
    "    leaves, self.tree = tree_flatten(params)\n",
    "    self.implicit_vars.update(tree_map(bm.TrainVar, leaves))\n",
    "\n",
    "    # rnn\n",
    "    self.rnn = bp.layers.GRU(256, 100)\n",
    "\n",
    "    # readout\n",
    "    self.linear = bp.layers.Dense(100, 10)\n",
    "\n",
    "  def update(self, sha, x):\n",
    "    params = tree_unflatten(self.tree, [v.value for v in self.implicit_vars.values()])\n",
    "    x = self.cnn.apply({'params': params}, bm.as_jax(x))\n",
    "    x = self.rnn(sha, x)\n",
    "    x = self.linear(sha, x)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We initialize the network, optimizer, loss function, and BP trainer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "net = Network()\n",
    "opt = bp.optim.Momentum(0.1)\n",
    "\n",
    "def loss_func(predictions, targets):\n",
    "  logits = bm.max(predictions, axis=1)\n",
    "  loss = bp.losses.cross_entropy_loss(logits, targets)\n",
    "  accuracy = bm.mean(bm.argmax(logits, -1) == targets)\n",
    "  return loss, {'accuracy': accuracy}\n",
    "\n",
    "trainer = bp.train.BPTT(net, loss_fun=loss_func, optimizer=opt, loss_has_aux=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get the MNIST dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "train_dataset = bp.datasets.MNIST(r'D:\\data\\mnist', train=True, download=True)\n",
    "X = train_dataset.data.reshape((-1, 7, 4, 28, 1)) / 255\n",
    "Y = train_dataset.targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, train our defined model by using ``BPTT.fit()`` function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 100 steps, use 32.5824 s, train loss 0.96465, accuracy 0.66015625\n",
      "Train 200 steps, use 30.9035 s, train loss 0.38974, accuracy 0.89453125\n",
      "Train 300 steps, use 33.1075 s, train loss 0.31525, accuracy 0.890625\n",
      "Train 400 steps, use 31.4062 s, train loss 0.23846, accuracy 0.91015625\n",
      "Train 500 steps, use 32.3371 s, train loss 0.21995, accuracy 0.9296875\n",
      "Train 600 steps, use 32.5692 s, train loss 0.20885, accuracy 0.92578125\n",
      "Train 700 steps, use 33.0139 s, train loss 0.24748, accuracy 0.90625\n",
      "Train 800 steps, use 31.9635 s, train loss 0.14563, accuracy 0.953125\n",
      "Train 900 steps, use 31.8845 s, train loss 0.17017, accuracy 0.94140625\n",
      "Train 1000 steps, use 32.0537 s, train loss 0.09413, accuracy 0.95703125\n",
      "Train 1100 steps, use 32.3714 s, train loss 0.06015, accuracy 0.984375\n",
      "Train 1200 steps, use 31.6957 s, train loss 0.12061, accuracy 0.94921875\n",
      "Train 1300 steps, use 31.8346 s, train loss 0.13908, accuracy 0.953125\n",
      "Train 1400 steps, use 31.5252 s, train loss 0.10718, accuracy 0.953125\n",
      "Train 1500 steps, use 31.7274 s, train loss 0.07869, accuracy 0.96875\n",
      "Train 1600 steps, use 32.3928 s, train loss 0.08295, accuracy 0.96875\n",
      "Train 1700 steps, use 31.7718 s, train loss 0.07569, accuracy 0.96484375\n",
      "Train 1800 steps, use 31.9243 s, train loss 0.08607, accuracy 0.9609375\n",
      "Train 1900 steps, use 32.2454 s, train loss 0.04332, accuracy 0.984375\n",
      "Train 2000 steps, use 31.6231 s, train loss 0.02369, accuracy 0.9921875\n",
      "Train 2100 steps, use 31.7800 s, train loss 0.03862, accuracy 0.9765625\n",
      "Train 2200 steps, use 31.5431 s, train loss 0.01871, accuracy 0.9921875\n",
      "Train 2300 steps, use 32.1064 s, train loss 0.03255, accuracy 0.9921875\n"
     ]
    }
   ],
   "source": [
    "trainer.fit([X, Y], batch_size=256, num_epoch=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}