{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55233d4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Automatic Differentiation for Class Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355bb9b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "@[Chaoming Wang](https://github.com/chaoming0625)\n",
    "@[Xiaoyu Chen](mailto:c-xy17@tsinghua.org.cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1e2d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this section, we are going to talk about how to realize automatic differentiation on your variables in a function or a class object. In current machine learning systems, gradients are commonly used in various situations. Therefore, we should understand:\n",
    "\n",
    "- How to calculate derivatives of arbitrary complex functions?\n",
    "- How to compute high-order gradients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ae6076",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "\n",
    "bp.math.set_platform('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa7421",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca8416",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Every autograd function in BrainPy has several keywords. All examples below are illustrated through [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst). Other autograd functions have the same settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75313f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ``argnums`` and ``grad_vars``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772965c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The autograd functions in BrainPy can compute derivatives of *function arguments* (specified by `argnums`) or *non-argument variables* (specified by ``grad_vars``). For instance, the following is a linear readout model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be17f596",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = bm.random.random((1, 10))\n",
    "        self.b = bm.zeros(1)\n",
    "    \n",
    "    def update(self, x):\n",
    "        r = bm.dot(self.w, x) + self.b\n",
    "        return r.sum()\n",
    "    \n",
    "l = Linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d93392",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we try to focus on the derivative of the argument \"x\" when calling the update function, we can set this through ``argnums``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf6ae1f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "JaxArray([0.9865978 , 0.14363837, 0.03861248, 0.42379665, 0.7038013 ,\n          0.11866355, 0.67538667, 0.15790391, 0.6050298 , 0.778468  ],            dtype=float32)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = bm.grad(l.update, argnums=0)\n",
    "\n",
    "grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beb97b5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By contrast, if you focus on the derivatives of parameters \"self.w\" and \"self.b\", we should label them with ``grad_vars``:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f0d2c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(DeviceArray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),\n DeviceArray([1.], dtype=float32))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = bm.grad(l.update, grad_vars=(l.w, l.b))\n",
    "\n",
    "grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea78df",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we pay attention to the derivatives of both argument \"x\" and parameters \"self.w\" and \"self.b\", ``argnums`` and ``grad_vars`` can be used together. In this condition, the gradient function will return gradients with the format of ``(var_grads, arg_grads)``, where ``arg_grads`` refers to the gradients of \"argnums\" and ``var_grads`` refers to the gradients of \"grad_vars\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc0347c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grad = bm.grad(l.update, grad_vars=(l.w, l.b), argnums=0)\n",
    "\n",
    "var_grads, arg_grads = grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6f0f99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(DeviceArray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),\n DeviceArray([1.], dtype=float32))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0d8b7f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "JaxArray([0.9865978 , 0.14363837, 0.03861248, 0.42379665, 0.7038013 ,\n          0.11866355, 0.67538667, 0.15790391, 0.6050298 , 0.778468  ],            dtype=float32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f20772",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ``return_value``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5b9dd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As is mentioned above, autograd functions return a function which computes gradients regardless of the returned value. Sometimes, however, we care about the value the function returns, not just the gradients. In this condition, you can set ``return_value=True`` in the autograd function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "600ea97e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grad = bm.grad(l.update, argnums=0, return_value=True)\n",
    "\n",
    "gradient, value = grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6909c04",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "JaxArray([0.9865978 , 0.14363837, 0.03861248, 0.42379665, 0.7038013 ,\n          0.11866355, 0.67538667, 0.15790391, 0.6050298 , 0.778468  ],            dtype=float32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "528b392f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray(4.6318984, dtype=float32)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f829bd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ``has_aux``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f4e2b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In some situations, we are interested in the intermediate values in a function, and ``has_aux=True`` can be of great help. The constraint is that you must return values with the format of ``(loss, aux_data)``. For instance, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e93b87",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LinearAux(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(LinearAux, self).__init__()\n",
    "        self.w = bm.random.random((1, 10))\n",
    "        self.b = bm.zeros(1)\n",
    "    \n",
    "    def update(self, x):\n",
    "        dot = bm.dot(self.w, x)\n",
    "        r = (dot + self.b).sum()\n",
    "        return r, (r, dot)  # here the aux data is a tuple, includes the loss and the dot value.\n",
    "                            # however, aux can be arbitrary complex.\n",
    "    \n",
    "l2 = LinearAux()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c683624",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grad = bm.grad(l2.update, argnums=0, has_aux=True)\n",
    "\n",
    "gradient, aux = grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "828ae73f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "JaxArray([0.20289445, 0.4745227 , 0.36053288, 0.94524395, 0.8360598 ,\n          0.06507981, 0.7748591 , 0.8377187 , 0.5767547 , 0.47604012],            dtype=float32)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d921e0d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(DeviceArray(5.5497055, dtype=float32), JaxArray([5.5497055], dtype=float32))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6becdd17",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When multiple keywords (``argnums``, ``grad_vars``, ``has_aux`` or``return_value``) are set simulatenously, the return format of the gradient function can be inspected through the corresponding API documentation [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b31f4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ``brainpy.math.grad()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289c868",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst) takes a function/object ($f : \\mathbb{R}^n \\to \\mathbb{R}$) as the input and returns a new function ($\\partial f(x) \\to \\mathbb{R}^n$) which computes the gradient of the original function/object. It's worthy to note that ``brainpy.math.grad()`` only supports returning scalar values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56075f51",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pure functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4cccc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For pure function, the gradient is taken with respect to the first argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45352485",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return a * 2 + b\n",
    "\n",
    "grad_f1 = bm.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6009405f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray(2., dtype=float32, weak_type=True)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f1(2., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f4f4e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "However, this can be controlled via the `argnums` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58aa6fbc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(DeviceArray(2., dtype=float32, weak_type=True),\n DeviceArray(1., dtype=float32, weak_type=True))"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f2 = bm.grad(f, argnums=(0, 1))\n",
    "\n",
    "grad_f2(2., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0874ef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Class objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00906f22",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For a class object or a class bound function, the gradient is taken with respect to the provided ``grad_vars`` and ``argnums`` setting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acc95d4c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class F(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(F, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c):\n",
    "        ab = self.a * self.b\n",
    "        ab2 = ab * 2\n",
    "        vv = ab2 + c\n",
    "        return vv.mean()\n",
    "    \n",
    "f = F()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d64bc3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The ``grad_vars`` can be a JaxArray, or a list/tuple/dict of JaxArray. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30484eab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'F0.a': DeviceArray([2.], dtype=float32),\n 'F0.b': DeviceArray([2.], dtype=float32)}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.grad(f, grad_vars=f.train_vars())(10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa99d3ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(DeviceArray([2.], dtype=float32), DeviceArray([2.], dtype=float32))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.grad(f, grad_vars=[f.a, f.b])(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847c77f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If there are dynamically changed values in the gradient function, you can provide them in the ``dyn_vars`` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f77b4c0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class F2(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(F2, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c):\n",
    "        ab = self.a * self.b\n",
    "        ab = ab * 2\n",
    "        self.a.value = ab\n",
    "        return (ab + c).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0cf62b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray([2.], dtype=float32)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = F2()\n",
    "bm.grad(f2, dyn_vars=[f2.a], grad_vars=f2.b)(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6998ec7c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Besides, if you are interested in the gradient of the input value, please use the ``argnums`` argument. Then, the gradient function will return ``(grads_of_grad_vars, grads_of_args)``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42c0dca2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class F3(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(F3, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c, d):\n",
    "        ab = self.a * self.b\n",
    "        ab = ab * 2\n",
    "        return (ab + c * d).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fe1c9ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads_of_gv : (DeviceArray([2.], dtype=float32), DeviceArray([2.], dtype=float32))\n",
      "grad_of_args : 3.0\n"
     ]
    }
   ],
   "source": [
    "f3 = F3()\n",
    "grads_of_gv, grad_of_args = bm.grad(f3, grad_vars=[f3.a, f3.b], argnums=0)(10., 3.)\n",
    "\n",
    "print(\"grads_of_gv :\", grads_of_gv)\n",
    "print(\"grad_of_args :\", grad_of_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba55cac6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads_of_gv : (DeviceArray([2.], dtype=float32), DeviceArray([2.], dtype=float32))\n",
      "grad_of_args : (DeviceArray(3., dtype=float32, weak_type=True), DeviceArray(10., dtype=float32, weak_type=True))\n"
     ]
    }
   ],
   "source": [
    "f3 = F3()\n",
    "grads_of_gv, grad_of_args = bm.grad(f3, grad_vars=[f3.a, f3.b], argnums=(0, 1))(10., 3.)\n",
    "\n",
    "print(\"grads_of_gv :\", grads_of_gv)\n",
    "print(\"grad_of_args :\", grad_of_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06491457",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Actually, it is recommended to provide all dynamically changed variables, whether or not they are updated in the gradient function, in the ``dyn_vars`` argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cedb4d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Auxiliary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469a67f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Usually, we want to get the loss value, or we want to return some intermediate variables during the gradient computation. In these situation, users can set ``has_aux=True`` to return auxiliary data and set ``return_value=True`` to return the loss value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a34a7e5a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  [2.]\n",
      "loss:  12.0\n"
     ]
    }
   ],
   "source": [
    "# return loss\n",
    "\n",
    "grad, loss = bm.grad(f, grad_vars=f.a, return_value=True)(10.)\n",
    "\n",
    "print('grad: ', grad)\n",
    "print('loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a1ad862",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  [1.]\n",
      "aux_data:  (JaxArray([1.], dtype=float32), JaxArray([2.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "class F4(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(F4, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c):\n",
    "        ab = self.a * self.b\n",
    "        ab2 = ab * 2\n",
    "        loss = (ab + c).mean()\n",
    "        return loss, (ab, ab2)\n",
    "    \n",
    "\n",
    "f4 = F4()\n",
    "    \n",
    "# return intermediate values\n",
    "grad, aux_data = bm.grad(f4, grad_vars=f4.a, has_aux=True)(10.)\n",
    "\n",
    "print('grad: ', grad)\n",
    "print('aux_data: ', aux_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2c322",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```note\n",
    "Any function used to compute gradients through ``brainpy.math.grad()`` must return a scalar value. Otherwise an error will raise. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea6a89f5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'TypeError'> Gradient only defined for scalar-output functions. Output had shape: (2,).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bm.grad(lambda x: x)(bm.zeros(2))\n",
    "except Exception as e:\n",
    "    print(type(e), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d08e3753",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "JaxArray([0.5, 0.5], dtype=float32)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is right\n",
    "\n",
    "bm.grad(lambda x: x.mean())(bm.zeros(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119967c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ``brainpy.math.vector_grad()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542356e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If users want to take gradients for a vector-output values, please use the [brainpy.math.vector_grad()](../apis/auto/math/generated/brainpy.math.autograd.vector_grad.rst) function. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a0a9b71",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def f(a, b): \n",
    "    return bm.sin(b) * a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb68361",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Gradients for vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1323e89d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vectors\n",
    "\n",
    "a = bm.arange(5.)\n",
    "b = bm.random.random(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a776e614",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "JaxArray([0.22263631, 0.19832121, 0.47522876, 0.40596786, 0.2040254 ],            dtype=float32)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(f)(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85748195",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(JaxArray([0.22263631, 0.19832121, 0.47522876, 0.40596786, 0.2040254 ],            dtype=float32),\n JaxArray([0.       , 0.9801371, 1.7597246, 2.741662 , 3.9158623], dtype=float32))"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(f, argnums=(0, 1))(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10694945",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Gradients for matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19acd682",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# matrix\n",
    "\n",
    "a = bm.arange(6.).reshape((2, 3))\n",
    "b = bm.random.random((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c049c25",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "JaxArray([[0.       , 0.8662993, 1.1221857],\n          [2.9322515, 2.3293345, 3.024507 ]], dtype=float32)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(f, argnums=1)(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "060fb4f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(JaxArray([[0.45055482, 0.49952534, 0.8277529 ],\n           [0.21131878, 0.8129499 , 0.79630035]], dtype=float32),\n JaxArray([[0.       , 0.8662993, 1.1221857],\n           [2.9322515, 2.3293345, 3.024507 ]], dtype=float32))"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(f, argnums=(0, 1))(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e96324",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Similar to [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst) , ``brainpy.math.vector_grad()`` also supports derivatives of variables in a class object. Here is a simple example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34e4f7bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Test(bp.Base):\n",
    "  def __init__(self):\n",
    "    super(Test, self).__init__()\n",
    "    self.x = bm.ones(5)\n",
    "    self.y = bm.ones(5)\n",
    "\n",
    "  def __call__(self):\n",
    "    return self.x ** 2 + self.y ** 3 + 10\n",
    "\n",
    "t = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91fb638c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray([2., 2., 2., 2., 2.], dtype=float32)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(t, grad_vars=t.x)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "678e2a24",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(DeviceArray([2., 2., 2., 2., 2.], dtype=float32),)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(t, grad_vars=(t.x, ))()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3279ad8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(DeviceArray([2., 2., 2., 2., 2.], dtype=float32),\n DeviceArray([3., 3., 3., 3., 3.], dtype=float32))"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(t, grad_vars=(t.x, t.y))()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9cb39",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Other operations like ``return_value`` and ``has_aux`` in [brainpy.math.vector_grad()](../apis/auto/math/generated/brainpy.math.autograd.vector_grad.rst)  are the same as those in [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca257d2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ``brainpy.math.jacobian()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68747a3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Another way to take gradients of a vector-output value is using [brainpy.math.jacobian()](../apis/auto/math/generated/brainpy.math.autograd.jacobian.rst). ``brainpy.math.jacobian()`` aims to automatically compute the Jacobian matrices $\\partial f(x) \\in \\mathbb{R}^{m \\times n}$ by the given function $f : \\mathbb{R}^n \\to \\mathbb{R}^m$ at the given point of $x \\in \\mathbb{R}^n$. Here, we will not go to the details of the implementation and usage of the ``brainpy.math.jacobian()``. Instead, we only show two examples about the pure function and class function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253df55c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Given the following function, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13ff570b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def f1(x, y):\n",
    "    a = 4 * x[1] ** 2 - 2 * x[2]\n",
    "    r = jnp.asarray([x[0] * y[0], 5 * x[2] * y[1], a, x[2] * jnp.sin(x[0])])\n",
    "    return r, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1aefb47d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_x = bm.array([1., 2., 3.])\n",
    "_y = bm.array([10., 5.])\n",
    "    \n",
    "grads, vec, aux = bm.jacobian(f1, return_value=True, has_aux=True)(_x, _y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6ea00cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "JaxArray([[10.        ,  0.        ,  0.        ],\n          [ 0.        ,  0.        , 25.        ],\n          [ 0.        , 16.        , -2.        ],\n          [ 1.6209068 ,  0.        ,  0.84147096]], dtype=float32)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c08984b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray([10.       , 75.       , 10.       ,  2.5244129], dtype=float32)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b64116c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray(10., dtype=float32)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad1eae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Given the following class objects,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f451a90",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Test(bp.Base):\n",
    "  def __init__(self):\n",
    "    super(Test, self).__init__()\n",
    "    self.x = bm.array([1., 2., 3.])\n",
    "\n",
    "  def __call__(self, y):\n",
    "    a = self.x[0] * y[0]\n",
    "    b = 5 * self.x[2] * y[1]\n",
    "    c = 4 * self.x[1] ** 2 - 2 * self.x[2]\n",
    "    d = self.x[2] * jnp.sin(self.x[0])\n",
    "    r = jnp.asarray([a, b, c, d])\n",
    "    return r, (c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f68ee77",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t = Test()\n",
    "f_grad = bm.jacobian(t, grad_vars=t.x, argnums=0, has_aux=True, return_value=True)\n",
    "\n",
    "(var_grads, arg_grads), value, aux = f_grad(_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3db0d7d1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray([[10.        ,  0.        ,  0.        ],\n             [ 0.        ,  0.        , 25.        ],\n             [ 0.        , 16.        , -2.        ],\n             [ 1.6209068 ,  0.        ,  0.84147096]], dtype=float32)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82547a2f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "JaxArray([[ 1.,  0.],\n          [ 0., 15.],\n          [ 0.,  0.],\n          [ 0.,  0.]], dtype=float32)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "382e1ab2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray([10.       , 75.       , 10.       ,  2.5244129], dtype=float32)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de401f68",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(DeviceArray(10., dtype=float32), DeviceArray(2.5244129, dtype=float32))"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a486499",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For more details on automatical differentation, please see our [API documentation](../apis/auto/math/autograd.rst)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}