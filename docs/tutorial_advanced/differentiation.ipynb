{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55233d4",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with `BrainPyObject`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355bb9b6",
   "metadata": {},
   "source": [
    "@[Chaoming Wang](https://github.com/chaoming0625)\n",
    "@[Xiaoyu Chen](mailto:c-xy17@tsinghua.org.cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1e2d7",
   "metadata": {},
   "source": [
    "In this section, we are going to talk about how to realize automatic differentiation on your variables in a function or a class object. In current machine learning systems, gradients are commonly used in various situations. Therefore, we should understand:\n",
    "\n",
    "- How to calculate derivatives of arbitrary complex functions?\n",
    "- How to compute high-order gradients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ae6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "\n",
    "# bm.set_platform('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa7421",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca8416",
   "metadata": {},
   "source": [
    "Every autograd function in BrainPy has several keywords. All examples below are illustrated through [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst). Other autograd functions have the same settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75313f",
   "metadata": {},
   "source": [
    "### ``argnums`` and ``grad_vars``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772965c3",
   "metadata": {},
   "source": [
    "The autograd functions in BrainPy can compute derivatives of *function arguments* (specified by `argnums`) or *non-argument variables* (specified by ``grad_vars``). For instance, the following is a linear readout model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be17f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(bp.BrainPyObject):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = bm.Variable(bm.random.random((1, 10)))\n",
    "        self.b = bm.Variable(bm.zeros(1))\n",
    "    \n",
    "    def update(self, x):\n",
    "        r = bm.dot(self.w, x) + self.b\n",
    "        return r.sum()\n",
    "    \n",
    "l = Linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d93392",
   "metadata": {},
   "source": [
    "If we try to focus on the derivative of the argument \"x\" when calling the update function, we can set this through ``argnums``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf6ae1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.82967794, 0.4514538 , 0.7641852 , 0.5760739 , 0.15079832,\n",
       "       0.67167246, 0.9960797 , 0.4768778 , 0.5323193 , 0.2970233 ],      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = bm.grad(l.update, argnums=0)\n",
    "\n",
    "grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beb97b5",
   "metadata": {},
   "source": [
    "By contrast, if you focus on the derivatives of parameters \"self.w\" and \"self.b\", we should label them with ``grad_vars``:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f0d2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),\n",
       " Array([1.], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = bm.grad(l.update, grad_vars=(l.w, l.b))\n",
    "\n",
    "grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea78df",
   "metadata": {},
   "source": [
    "If we pay attention to the derivatives of both argument \"x\" and parameters \"self.w\" and \"self.b\", ``argnums`` and ``grad_vars`` can be used together. In this condition, the gradient function will return gradients with the format of ``(var_grads, arg_grads)``, where ``arg_grads`` refers to the gradients of \"argnums\" and ``var_grads`` refers to the gradients of \"grad_vars\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc0347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = bm.grad(l.update, grad_vars=(l.w, l.b), argnums=0)\n",
    "\n",
    "var_grads, arg_grads = grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6f0f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),\n",
       " Array([1.], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0d8b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.82967794, 0.4514538 , 0.7641852 , 0.5760739 , 0.15079832,\n",
       "       0.67167246, 0.9960797 , 0.4768778 , 0.5323193 , 0.2970233 ],      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f20772",
   "metadata": {},
   "source": [
    "### ``return_value``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5b9dd",
   "metadata": {},
   "source": [
    "As is mentioned above, autograd functions return a function which computes gradients regardless of the returned value. Sometimes, however, we care about the value the function returns, not just the gradients. In this condition, you can set ``return_value=True`` in the autograd function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "600ea97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = bm.grad(l.update, argnums=0, return_value=True)\n",
    "\n",
    "gradient, value = grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6909c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.82967794, 0.4514538 , 0.7641852 , 0.5760739 , 0.15079832,\n",
       "       0.67167246, 0.9960797 , 0.4768778 , 0.5323193 , 0.2970233 ],      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "528b392f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(5.746162, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f829bd",
   "metadata": {},
   "source": [
    "### ``has_aux``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f4e2b",
   "metadata": {},
   "source": [
    "In some situations, we are interested in the intermediate values in a function, and ``has_aux=True`` can be of great help. The constraint is that you must return values with the format of ``(loss, aux_data)``. For instance, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e93b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAux(bp.BrainPyObject):\n",
    "    def __init__(self):\n",
    "        super(LinearAux, self).__init__()\n",
    "        self.w = bm.Variable(bm.random.random((1, 10)))\n",
    "        self.b = bm.Variable(bm.zeros(1))\n",
    "    \n",
    "    def update(self, x):\n",
    "        dot = bm.dot(self.w, x)\n",
    "        r = (dot + self.b).sum()\n",
    "        return r, (r, dot)  # here the aux data is a tuple, includes the loss and the dot value.\n",
    "                            # however, aux can be arbitrary complex.\n",
    "    \n",
    "l2 = LinearAux()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c683624",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = bm.grad(l2.update, argnums=0, has_aux=True)\n",
    "\n",
    "gradient, aux = grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "828ae73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.00531888, 0.15332317, 0.21680796, 0.49158132, 0.94955766,\n",
       "       0.12618113, 0.9718574 , 0.943076  , 0.33870268, 0.9838389 ],      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d921e0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(5.1802454, dtype=float32), Array([5.1802454], dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6becdd17",
   "metadata": {},
   "source": [
    "When multiple keywords (``argnums``, ``grad_vars``, ``has_aux`` or``return_value``) are set simulatenously, the return format of the gradient function can be inspected through the corresponding API documentation [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b31f4",
   "metadata": {},
   "source": [
    "## ``brainpy.math.grad()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289c868",
   "metadata": {},
   "source": [
    "[brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst) takes a function/object ($f : \\mathbb{R}^n \\to \\mathbb{R}$) as the input and returns a new function ($\\partial f(x) \\to \\mathbb{R}^n$) which computes the gradient of the original function/object. It's worthy to note that ``brainpy.math.grad()`` only supports returning scalar values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56075f51",
   "metadata": {},
   "source": [
    "### Pure functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4cccc",
   "metadata": {},
   "source": [
    "For pure function, the gradient is taken with respect to the first argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45352485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return a * 2 + b\n",
    "\n",
    "grad_f1 = bm.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6009405f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f1(2., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f4f4e",
   "metadata": {},
   "source": [
    "However, this can be controlled via the `argnums` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58aa6fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(2., dtype=float32, weak_type=True),\n",
       " Array(1., dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f2 = bm.grad(f, argnums=(0, 1))\n",
    "\n",
    "grad_f2(2., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0874ef",
   "metadata": {},
   "source": [
    "### Class objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00906f22",
   "metadata": {},
   "source": [
    "For a class object or a class bound function, the gradient is taken with respect to the provided ``grad_vars`` and ``argnums`` setting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acc95d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F(bp.BrainPyObject):\n",
    "    def __init__(self):\n",
    "        super(F, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c):\n",
    "        ab = self.a * self.b\n",
    "        ab2 = ab * 2\n",
    "        vv = ab2 + c\n",
    "        return vv.mean()\n",
    "    \n",
    "f = F()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d64bc3",
   "metadata": {},
   "source": [
    "The ``grad_vars`` can be a Array, or a list/tuple/dict of Array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30484eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F0.a': Array([2.], dtype=float32), 'F0.b': Array([2.], dtype=float32)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.grad(f, grad_vars=f.train_vars())(10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa99d3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([2.], dtype=float32), Array([2.], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.grad(f, grad_vars=[f.a, f.b])(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847c77f",
   "metadata": {},
   "source": [
    "If there are dynamically changed values in the gradient function, you can provide them in the ``dyn_vars`` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f77b4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F2(bp.BrainPyObject):\n",
    "    def __init__(self):\n",
    "        super(F2, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c):\n",
    "        ab = self.a * self.b\n",
    "        ab = ab * 2\n",
    "        self.a.value = ab\n",
    "        return (ab + c).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0cf62b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = F2()\n",
    "bm.grad(f2, dyn_vars=[f2.a], grad_vars=f2.b)(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6998ec7c",
   "metadata": {},
   "source": [
    "Besides, if you are interested in the gradient of the input value, please use the ``argnums`` argument. Then, the gradient function will return ``(grads_of_grad_vars, grads_of_args)``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42c0dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F3(bp.BrainPyObject):\n",
    "    def __init__(self):\n",
    "        super(F3, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c, d):\n",
    "        ab = self.a * self.b\n",
    "        ab = ab * 2\n",
    "        return (ab + c * d).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fe1c9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads_of_gv : (Array([2.], dtype=float32), Array([2.], dtype=float32))\n",
      "grad_of_args : 3.0\n"
     ]
    }
   ],
   "source": [
    "f3 = F3()\n",
    "grads_of_gv, grad_of_args = bm.grad(f3, grad_vars=[f3.a, f3.b], argnums=0)(10., 3.)\n",
    "\n",
    "print(\"grads_of_gv :\", grads_of_gv)\n",
    "print(\"grad_of_args :\", grad_of_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba55cac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads_of_gv : (Array([2.], dtype=float32), Array([2.], dtype=float32))\n",
      "grad_of_args : (Array(3., dtype=float32, weak_type=True), Array(10., dtype=float32, weak_type=True))\n"
     ]
    }
   ],
   "source": [
    "f3 = F3()\n",
    "grads_of_gv, grad_of_args = bm.grad(f3, grad_vars=[f3.a, f3.b], argnums=(0, 1))(10., 3.)\n",
    "\n",
    "print(\"grads_of_gv :\", grads_of_gv)\n",
    "print(\"grad_of_args :\", grad_of_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06491457",
   "metadata": {},
   "source": [
    "Actually, it is recommended to provide all dynamically changed variables, whether or not they are updated in the gradient function, in the ``dyn_vars`` argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cedb4d",
   "metadata": {},
   "source": [
    "### Auxiliary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469a67f",
   "metadata": {},
   "source": [
    "Usually, we want to get the loss value, or we want to return some intermediate variables during the gradient computation. In these situation, users can set ``has_aux=True`` to return auxiliary data and set ``return_value=True`` to return the loss value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a34a7e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  [2.]\n",
      "loss:  12.0\n"
     ]
    }
   ],
   "source": [
    "# return loss\n",
    "\n",
    "grad, loss = bm.grad(f, grad_vars=f.a, return_value=True)(10.)\n",
    "\n",
    "print('grad: ', grad)\n",
    "print('loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a1ad862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  [1.]\n",
      "aux_data:  (Array([1.], dtype=float32), Array([2.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "class F4(bp.BrainPyObject):\n",
    "    def __init__(self):\n",
    "        super(F4, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c):\n",
    "        ab = self.a * self.b\n",
    "        ab2 = ab * 2\n",
    "        loss = (ab + c).mean()\n",
    "        return loss, (ab, ab2)\n",
    "    \n",
    "\n",
    "f4 = F4()\n",
    "    \n",
    "# return intermediate values\n",
    "grad, aux_data = bm.grad(f4, grad_vars=f4.a, has_aux=True)(10.)\n",
    "\n",
    "print('grad: ', grad)\n",
    "print('aux_data: ', aux_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2c322",
   "metadata": {},
   "source": [
    "```note\n",
    "Any function used to compute gradients through ``brainpy.math.grad()`` must return a scalar value. Otherwise an error will raise. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea6a89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'TypeError'> Gradient only defined for scalar-output functions. Output had shape: (2,).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bm.grad(lambda x: x)(bm.zeros(2))\n",
    "except Exception as e:\n",
    "    print(type(e), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d08e3753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.5, 0.5], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is right\n",
    "\n",
    "bm.grad(lambda x: x.mean())(bm.zeros(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119967c0",
   "metadata": {},
   "source": [
    "## ``brainpy.math.vector_grad()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542356e",
   "metadata": {},
   "source": [
    "If users want to take gradients for a vector-output values, please use the [brainpy.math.vector_grad()](../apis/auto/math/generated/brainpy.math.autograd.vector_grad.rst) function. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a0a9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b): \n",
    "    return bm.sin(b) * a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb68361",
   "metadata": {},
   "source": [
    "#### Gradients for vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1323e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors\n",
    "\n",
    "a = bm.arange(5.)\n",
    "b = bm.random.random(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a776e614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.6534876 , 0.41332984, 0.31663823, 0.28691947, 0.6181594 ],      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(f)(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85748195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([0.6534876 , 0.41332984, 0.31663823, 0.28691947, 0.6181594 ],      dtype=float32),\n",
       " Array([0.        , 0.91058135, 1.8970927 , 2.8738642 , 3.144211  ],      dtype=float32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(f, argnums=(0, 1))(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10694945",
   "metadata": {},
   "source": [
    "#### Gradients for matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19acd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix\n",
    "\n",
    "a = bm.arange(6.).reshape((2, 3))\n",
    "b = bm.random.random((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c049c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.       , 0.9752406, 1.1096077],\n",
       "       [2.2716377, 3.94111  , 4.671802 ]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(f, argnums=1)(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "060fb4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0.5042689 , 0.22114661, 0.8319812 ],\n",
       "        [0.6531685 , 0.17096272, 0.3563294 ]], dtype=float32),\n",
       " Array([[0.       , 0.9752406, 1.1096077],\n",
       "        [2.2716377, 3.94111  , 4.671802 ]], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(f, argnums=(0, 1))(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e96324",
   "metadata": {},
   "source": [
    "Similar to [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst) , ``brainpy.math.vector_grad()`` also supports derivatives of variables in a class object. Here is a simple example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34e4f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(bp.BrainPyObject):\n",
    "  def __init__(self):\n",
    "    super(Test, self).__init__()\n",
    "    self.x = bm.Variable(bm.ones(5))\n",
    "    self.y = bm.Variable(bm.ones(5))\n",
    "\n",
    "  def __call__(self):\n",
    "    return self.x ** 2 + self.y ** 3 + 10\n",
    "\n",
    "t = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91fb638c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(t, grad_vars=t.x)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "678e2a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([2., 2., 2., 2., 2.], dtype=float32),)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(t, grad_vars=(t.x, ))()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3279ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([2., 2., 2., 2., 2.], dtype=float32),\n",
       " Array([3., 3., 3., 3., 3.], dtype=float32))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(t, grad_vars=(t.x, t.y))()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9cb39",
   "metadata": {},
   "source": [
    "Other operations like ``return_value`` and ``has_aux`` in [brainpy.math.vector_grad()](../apis/auto/math/generated/brainpy.math.autograd.vector_grad.rst)  are the same as those in [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca257d2",
   "metadata": {},
   "source": [
    "## ``brainpy.math.jacobian()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68747a3",
   "metadata": {},
   "source": [
    "Another way to take gradients of a vector-output value is using [brainpy.math.jacobian()](../apis/auto/math/generated/brainpy.math.autograd.jacobian.rst). ``brainpy.math.jacobian()`` aims to automatically compute the Jacobian matrices $\\partial f(x) \\in \\mathbb{R}^{m \\times n}$ by the given function $f : \\mathbb{R}^n \\to \\mathbb{R}^m$ at the given point of $x \\in \\mathbb{R}^n$. Here, we will not go to the details of the implementation and usage of the ``brainpy.math.jacobian()``. Instead, we only show two examples about the pure function and class function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253df55c",
   "metadata": {},
   "source": [
    "Given the following function, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13ff570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def f1(x, y):\n",
    "    a = 4 * x[1] ** 2 - 2 * x[2]\n",
    "    r = jnp.asarray([x[0] * y[0], 5 * x[2] * y[1], a, x[2] * jnp.sin(x[0])])\n",
    "    return r, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1aefb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = bm.array([1., 2., 3.])\n",
    "_y = bm.array([10., 5.])\n",
    "    \n",
    "grads, vec, aux = bm.jacobian(f1, return_value=True, has_aux=True)(_x, _y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6ea00cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[10.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        , 25.        ],\n",
       "       [ 0.        , 16.        , -2.        ],\n",
       "       [ 1.6209068 ,  0.        ,  0.84147096]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c08984b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([10.       , 75.       , 10.       ,  2.5244129], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b64116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(10., dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad1eae",
   "metadata": {},
   "source": [
    "Given the following class objects,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f451a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(bp.BrainPyObject):\n",
    "  def __init__(self):\n",
    "    super(Test, self).__init__()\n",
    "    self.x = bm.Variable(bm.array([1., 2., 3.]))\n",
    "\n",
    "  def __call__(self, y):\n",
    "    a = self.x[0] * y[0]\n",
    "    b = 5 * self.x[2] * y[1]\n",
    "    c = 4 * self.x[1] ** 2 - 2 * self.x[2]\n",
    "    d = self.x[2] * jnp.sin(self.x[0])\n",
    "    r = jnp.asarray([a, b, c, d])\n",
    "    return r, (c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f68ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Test()\n",
    "f_grad = bm.jacobian(t, grad_vars=t.x, argnums=0, has_aux=True, return_value=True)\n",
    "\n",
    "(var_grads, arg_grads), value, aux = f_grad(_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3db0d7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[10.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        , 25.        ],\n",
       "       [ 0.        , 16.        , -2.        ],\n",
       "       [ 1.6209068 ,  0.        ,  0.84147096]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82547a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.,  0.],\n",
       "       [ 0., 15.],\n",
       "       [ 0.,  0.],\n",
       "       [ 0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "382e1ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([10.       , 75.       , 10.       ,  2.5244129], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de401f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(10., dtype=float32), Array(2.5244129, dtype=float32))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a486499",
   "metadata": {},
   "source": [
    "For more details on automatical differentation, please see our [API documentation](../apis/auto/math/autograd.rst)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
