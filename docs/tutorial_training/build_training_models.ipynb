{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building Training Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, we are going to talk about how to build models for training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.3.0'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp.__version__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use built-in models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "``brainpy.dyn.DynamicalSystem`` provided in BrainPy can be used for model training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ``mode`` settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some built-in models have implemented the training interface for their training. Users can instantiate these models by providing the parameter ``mode=brainpy.modes.training`` for training model customization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For example, ``brainpy.neurons.LIF`` is a model commonly used in computational simulation, but it can also be used in training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": "NormalMode"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a LIF model for simulation\n",
    "\n",
    "lif = bp.neurons.LIF(1)\n",
    "lif.mode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "TrainingMode"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a LIF model for training.\n",
    "# In this mode, the model implement variables and functions\n",
    "# compatible with BrainPy's training interface.\n",
    "\n",
    "lif = bp.neurons.LIF(1, mode=bp.modes.training)\n",
    "lif.mode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "But some build-in models does not support training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NotImplementedError'> NVAR does not support TrainingMode. We only support BatchingMode, NormalMode. \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bp.layers.NVAR(1, 1, mode=bp.modes.training)\n",
    "except Exception as e:\n",
    "    print(type(e), e)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The ``mode`` can be used to control the weight types. Let's take a synaptic model for another example. For a non-trainable dense layer, the *weights* and *bias* are Array instances.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Array([[-0.00243703,  0.13264121, -0.52311915, -0.11253127],\n       [-0.5497899 , -0.81344956,  0.22661772, -0.22852308],\n       [ 0.9893868 ,  0.3720948 , -0.21082176, -0.84807736]],      dtype=float32)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = bp.layers.Dense(3, 4, mode=bp.modes.batching)\n",
    "\n",
    "l.W"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "TrainVar([[-0.3726533 ,  0.12259734,  0.80122465,  0.7158612 ],\n          [ 1.0421001 ,  0.48521557, -0.16323341,  0.02954347],\n          [-0.7722217 ,  0.16433354, -0.20813051,  0.71114105]],      dtype=float32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = bp.layers.Dense(3, 4, mode=bp.modes.training)\n",
    "\n",
    "l.W"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moreover, for some recurrent models, e.g., ``LSTM`` or ``GRU``, the ``state`` can be set to be trainable or not trainable by ``train_state`` argument. When setting ``train_state=True`` for the recurrent instance, a new attribute *.state2train* will be created."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adadu\\miniconda3\\envs\\brainpy\\lib\\site-packages\\brainpy-2.3.0-py3.9.egg\\brainpy\\dyn\\layers\\rnncells.py:419: UserWarning: Use \"brainpy.layers.RNNCell\" instead. \"brainpy.layers.VanillaRNN\" is deprecated and will be removed since 2.4.0.\n",
      "  warnings.warn('Use \"brainpy.layers.RNNCell\" instead. '\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainVar([0., 0., 0.], dtype=float32)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = bp.layers.VanillaRNN(1, 3, train_state=True)\n",
    "\n",
    "rnn.state2train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note the difference between the *.state2train* and the original *.state*:\n",
    "\n",
    "1. *.state2train* has no batch axis.\n",
    "2. When using `node.reset_state()` function, all values in the *.state* will be filled with *.state2train*."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Variable([[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]], dtype=float32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.reset_state(batch_size=5)\n",
    "rnn.state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naming a node"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For convenience, you can name a layer by specifying the name keyword argument:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Dense(name=hidden_layer, num_in=128, num_out=100, mode=TrainingMode)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp.layers.Dense(128, 100, name='hidden_layer')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initializing parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Many models have their parameters. We can set the parameter of a model with the following methods."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Arrays**\n",
    "\n",
    "If an array is provided, this is used unchanged as the parameter variable. For example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 50)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = bp.layers.Dense(10, 50, W_initializer=bm.random.normal(0, 0.01, size=(10, 50)))\n",
    "\n",
    "l.W.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Callable function**\n",
    "\n",
    "If a callable function (which receives a ``shape`` argument) is provided, the callable will be called with the desired shape to generate suitable initial parameter values. The variable is then initialized with those values. For example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(20, 30)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init(shape):\n",
    "    return bm.random.random(shape)\n",
    "\n",
    "l = bp.layers.Dense(20, 30, W_initializer=init)\n",
    "\n",
    "l.W.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Instance of** ``brainpy.init.Initializer``\n",
    "\n",
    "If a ``brainpy.init.Initializer`` instance is provided, the initial parameter values will be generated with the desired shape by using the Initializer instance. For example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(20, 30)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = bp.layers.Dense(20, 30, W_initializer=bp.init.Normal(0.01))\n",
    "\n",
    "l.W.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The weight matrix $W$ of this dense layer will be initialized using samples from a normal distribution with standard deviation 0.01 (see [brainpy.init](../apis/auto/initialize.rst) for more information)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **None parameter**\n",
    "\n",
    "Some types of parameter variables can also be set to ``None`` at initialization (e.g. biases). In that case, the parameter variable will be omitted. For example, creating a dense layer without biases is done as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "l = bp.layers.Dense(20, 100, b_initializer=None)\n",
    "\n",
    "print(l.b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Customize your models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Customizing your training models is simple. You just need to subclass ``brainpy.dyn.DynamicalSystem``, and implement its ``update()`` and ``reset_state()`` functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we demonstrate the model customization using two examples. The first is a recurrent layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class RecurrentLayer(bp.dyn.DynamicalSystem):\n",
    "    def __init__(self, num_in, num_out):\n",
    "        super(RecurrentLayer, self).__init__()\n",
    "\n",
    "        # define parameters\n",
    "        self.num_in = num_in\n",
    "        self.num_out = num_out\n",
    "\n",
    "        # define variables\n",
    "        self.state = bm.Variable(bm.zeros(1, num_out), batch_axis=0)\n",
    "\n",
    "        # define weights\n",
    "        self.win = bm.TrainVar(bm.random.normal(0., 1./num_in ** 0.5, size=(num_in, num_out)))\n",
    "        self.wrec = bm.TrainVar(bm.random.normal(0., 1./num_out ** 0.5, size=(num_out, num_out)))\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        # this function defines how to reset the mode states\n",
    "        self.state.value = bm.zeros((batch_size, self.num_out))\n",
    "\n",
    "    def update(self, sha, x):\n",
    "        # this function defined how the model update its state and produce its output\n",
    "        out = bm.dot(x, self.win) + bm.dot(self.state, self.wrec)\n",
    "        self.state.value = bm.tanh(out)\n",
    "        return self.state.value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This simple example illustrates many features essential for a training model. ``reset_state()`` function defines how to reset model states, which will be called at the first time step; ``update()`` function defines how the model states are evolving, which will be called at every time step."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another example is the dropout layer, which can be useful to demonstrate how to define a model with multiple behaviours."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class Dropout(bp.dyn.DynamicalSystem):\n",
    "  def __init__(self, prob: float, seed: int = None, name: str = None):\n",
    "    super(Dropout, self).__init__(name=name)\n",
    "    self.prob = prob\n",
    "    self.rng = bm.random.RandomState(seed=seed)\n",
    "\n",
    "  def update(self, sha, x):\n",
    "    if sha.get('fit', True):\n",
    "      keep_mask = self.rng.bernoulli(self.prob, x.shape)\n",
    "      return bm.where(keep_mask, x / self.prob, 0.)\n",
    "    else:\n",
    "      return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, the model makes different outputs according to the different values of a shared parameter ``fit``.\n",
    "\n",
    "You can define your own shared parameters, and then provide their shared parameters when calling the trainer objects (see the following section)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examples of training models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following, we illustrate several examples to build a trainable neural network model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Artificial neural networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "BrainPy provides neural network layers which can be useful to define artificial neural networks.\n",
    "\n",
    "Here, let's define a deep RNN model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class DeepRNN(bp.dyn.DynamicalSystem):\n",
    "    def __init__(self, num_in, num_recs, num_out):\n",
    "        super(DeepRNN, self).__init__()\n",
    "\n",
    "        self.l1 = bp.layers.LSTM(num_in, num_recs[0])\n",
    "        self.d1 = bp.layers.Dropout(0.2)\n",
    "        self.l2 = bp.layers.LSTM(num_recs[0], num_recs[1])\n",
    "        self.d2 = bp.layers.Dropout(0.2)\n",
    "        self.l3 = bp.layers.LSTM(num_recs[1], num_recs[2])\n",
    "        self.d3 = bp.layers.Dropout(0.2)\n",
    "        self.l4 = bp.layers.LSTM(num_recs[2], num_recs[3])\n",
    "        self.d4 = bp.layers.Dropout(0.2)\n",
    "        self.lout = bp.layers.Dense(num_recs[3], num_out)\n",
    "\n",
    "    def update(self, sha, x):\n",
    "        x = self.d1(sha, self.l1(sha, x))\n",
    "        x = self.d2(sha, self.l2(sha, x))\n",
    "        x = self.d3(sha, self.l3(sha, x))\n",
    "        x = self.d4(sha, self.l4(sha, x))\n",
    "        return self.lout(sha, x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note here the difference of the model building from PyTorch is that the first argument in ``update()`` function should be the shared parameters ``sha`` (i.e., these parameters are shared across all models, like the time ``t``, the running index ``i``, and the model running phase ``fit``). Then other individual arguments can all be customized by users. The details of the model definition specification can be seen in ????"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moreover, it is worthy to note that this model only defines the one step updating rule of how the model evolves according to the input ``x``."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reservoir computing models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we define a reservoir computing model called [next generation reservoir computing](https://doi.org/10.1038/s41467-021-25801-2) by using the built-in models provided in BrainPy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class NGRC(bp.dyn.DynamicalSystem):\n",
    "  def __init__(self, num_in, num_out):\n",
    "    super(NGRC, self).__init__()\n",
    "    self.r = bp.layers.NVAR(num_in, delay=4, order=2, stride=5,\n",
    "                            mode=bp.modes.batching)\n",
    "    self.o = bp.layers.Dense(self.r.num_out, num_out, mode=bp.modes.training)\n",
    "\n",
    "  def update(self, sha, x):\n",
    "    return self.o(sha, self.r(sha, x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above model, ``brainpy.layers.NVAR`` is a nonlinear vector autoregression machine, which does not have the training features. Therefore, we define its ``mode`` as batching mode. On the contrary, ``brainpy.layers.Dense`` has the trainable weights for model training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Spiking Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building trainable spiking neural networks in BrainPy is also a piece of cake. We provided commonly used spiking models for traditional dynamics simulation. But most of them can be used for training too.\n",
    "\n",
    "In the following, we provide an implementation of spiking neural networks in [(Neftci, Mostafa, & Zenke, 2019)](https://doi.org/10.1109/MSP.2019.2931595) for surrogate gradient learning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class SNN(bp.dyn.Network):\n",
    "  def __init__(self, num_in, num_rec, num_out):\n",
    "    super(SNN, self).__init__()\n",
    "\n",
    "    # neuron groups\n",
    "    self.i = bp.neurons.InputGroup(num_in, mode=bp.modes.training)\n",
    "    self.r = bp.neurons.LIF(num_rec, tau=10, V_reset=0, V_rest=0, V_th=1., mode=bp.modes.training)\n",
    "    self.o = bp.neurons.LeakyIntegrator(num_out, tau=5, mode=bp.modes.training)\n",
    "\n",
    "    # synapse: i->r\n",
    "    self.i2r = bp.synapses.Exponential(self.i, self.r, bp.conn.All2All(),\n",
    "                                       output=bp.synouts.CUBA(), tau=10.,\n",
    "                                       g_max=bp.init.KaimingNormal(scale=20.),\n",
    "                                       mode=bp.modes.training)\n",
    "    # synapse: r->o\n",
    "    self.r2o = bp.synapses.Exponential(self.r, self.o, bp.conn.All2All(),\n",
    "                                       output=bp.synouts.CUBA(), tau=10.,\n",
    "                                       g_max=bp.init.KaimingNormal(scale=20.),\n",
    "                                       mode=bp.modes.training)\n",
    "\n",
    "  def update(self, tdi, spike):\n",
    "    self.i2r(tdi, spike)\n",
    "    self.r2o(tdi)\n",
    "    self.r(tdi)\n",
    "    self.o(tdi)\n",
    "    return self.o.V.value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note here the mode in all models are specified as ``brainpy.modes.TrainingMode``."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
